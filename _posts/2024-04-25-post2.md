# GPU Batch Size for Deep Learning
In my initial exploration of training deep learning models using fastai I experimented with varied batch sizes for training. I'd like to share my results as they were initially surprising and some information about batch size.

Content covered in this riveting post:

1. TOC
{:toc}

## What is batch size?

Batch size refers to the number of samples being provided to the model for training at a time. This has a direct impact on the time taken to train a model as well as the error rate of the model.

## My Training Results

| Batch Size | 16 | 32 | 64 | 128 | 256 |
|----|----|----|----|----|----|
| Time [s] | 11.4 | 9.3 | 16.4 | 19.6 | 40.3 |

## Applications and Impact

ResNet18, along with its larger variants, has become a go-to choice for various computer vision tasks, including image classification, object detection, and segmentation. Its robustness, efficiency, and ease of training make it a versatile tool for both researchers and practitioners alike.

Beyond its practical applications, ResNet18 has had a profound impact on the development of subsequent architectures. Many state-of-the-art models, such as DenseNet, Wide ResNet, and SENet, draw inspiration from ResNet's fundamental insights, further propelling the field of deep learning forward.

## References
1. Yashvi Chandola, Jitendra Virmani, H.S. Bhadauria, Papendra Kumar,
Chapter 4 - End-to-end pre-trained CNN-based computer-aided classification system design for chest radiographs,
Editor(s): Yashvi Chandola, Jitendra Virmani, H.S. Bhadauria, Papendra Kumar,
In Primers in Biomedical Imaging Devices and Systems,
Deep Learning for Chest Radiographs,
Academic Press,
2021,
Pages 117-140,
ISBN 9780323901840,
https://doi.org/10.1016/B978-0-323-90184-0.00011-4.
(https://www.sciencedirect.com/science/article/pii/B9780323901840000114)

